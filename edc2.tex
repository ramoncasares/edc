% edc2.tex (RMCG20050518)


\Section La evolución

La teoría de la ^evolución^ de \[Darwin] ratificó esta idea filosófica
de \[Aristóteles], aunque rechazó otras de sus ideas biológicas. Esto
reafirma nuestra observación de que la ^filosofía^ tiene más futuro que
la ^ciencia^, porque sus teorías son más duraderas.

Según la teoría darviniana, lo que da identidad a la ^vida^ a lo largo
del tiempo es el proceso de ^evolución^. Y lo que evoluciona no es el
individuo, que sólo envejece, ni la ^especie^, aunque sea el primer
candidato. El caso es que la propia teoría de la evolución fue quien
apeó de su trono al concepto de especie.

Antes de \[Darwin], el mayor logro de la ^biología^ fue la ^taxonomía^
del sueco \[Linneo], que ordenaba en un enorme cuadro esquemático a
todas las especies vivas. Básicamente, los ^perros^, a pesar de las
enormes diferencias que existen entre sus razas, forman una única
especie porque pueden cruzarse entre sí y tener una descendencia viable.
De aquí se puede derivar una definición operacional de especie.

Es claro que los hijos pertenecen a la misma especie que los padres. Los
hijos de una pareja de perros son perros. Pero, según la teoría de la
evolución, descendemos de organismos unicelulares. Es decir, hay una
cadena continua que va ascendiendo de cada individuo a su madre, o a su
padre, y que nos conecta a nosotros, que pertenecemos según la
nomenclatura binomial popularizada por \[Linneo] a la especie
\latin{^homo sapiens^}, con ciertos organismos unicelulares que vivieron
hace mil millones de años. Me parece que también es claro que no sería
viable el cruce de un \latin{homo sapiens} con un organismo unicelular.

Así que `especie' es un concepto que sirve para cada momento concreto,
pero que no puede utilizarse a lo largo del tiempo. Esto mismo podemos
pontificarlo así: `especie' es un concepto ^sincrónico^, y no
^diacrónico^. Usar ^jerga^ para impresionar al profano es una táctica
empleada por todos los gremios, que también usamos los filósofos con
profusión, deleite y aprovechamiento. En cualquier caso, y dígase como
se diga, lo que no podemos decir con propiedad, es que una especie
evoluciona.

Así como en el ^restaurante^ lo que se mantenía era aquello que
atravesaba la cocina, en el caso de la vida lo que perdura ha de ser
aquello que pasa entre generaciones. Y la evolución darviniana nos
señala con precisión el punto de conexión de cada individuo vivo con el
resto de la vida. Es el material genético que recibimos al nacer. Es el
huevo fecundado que contiene la ^información^ genética, o sea, los
^genes^ que, como nos enseñó \[Aristóteles], son mera potencia
actualizable. La conclusión es ahora fácil: lo que evoluciona es la
información genética.

Si el ^universo^ completo es información, entonces lo que evoluciona ha
de ser información. Parece que con esta inferencia podíamos habernos
ahorrado alguna tinta. Yo no lo creo, sobre todo, porque la ^vida^
podría haber sido otra cosa, y entonces tendríamos que rehacer nuestra
teoría. De manera que establecer que la vida es información refuerza
nuestra creencia de que el universo es información, mientras que deducir
lo particular de lo general no aportaría nada. Una ^inferencia^ no añade
^conocimiento^, simplemente reordena lo que ya se sabía.


\Section La entropía

El otro logro científico del siglo {\sc xix} que también está
relacionado con la ^información^ proviene de la ^termodinámica^, que es
la rama de la ^física^ que estudia el ^calor^. En concreto, me refiero a
la ^segunda ley de la termodinámica^ que reza: la entropía de un sistema
cerrado nunca disminuye. La ^entropía^ mide el ^desorden^, y un ^sistema
cerrado^, que no aislado, intercambia ^energía^ y calor con su entorno,
pero no ^materia^.

La segunda ley de la termodinámica explica que, por sí mismo, el ^orden^
nunca crece. Nos dice que el orden no es gratuito, sino que cuesta
esfuerzo. Se han querido extraer muchas consecuencias filosóficas de
esta ley, sobre todo porque la ^vida^ parece violarla. Pero, por un
lado, aunque es cierto que la vida es un proceso que incrementa el
orden, resulta que la vida no es un sistema cerrado, sino
_abierto<sistema abierto>, ya que intercambia materia con su entorno, y,
por lo tanto, no se le aplica esta ley. Y, por el otro lado, no parece
que la ley tenga un significado muy profundo.

Sucede que, en cuanto el número de partes de un sistema no es muy
pequeño, el número de estados ordenados es un infinitésimo del número
total de estados posibles. Piense en la pequeñísima ^probabilidad^ que
existe de que, tras barajar, queden ordenados los ^naipes^. Y esto es
justo lo que dice esta ley, que barajando el desorden crece. Por eso,
para jugar a las cartas, se baraja. Es decir, la segunda ley de la
termodinámica se limita a constatar que el orden es mucho más
infrecuente que el desorden. Y ahora ya podemos parafrasear la segunda
ley de la _termodinámica<tautología>, sin desvirtuarla, así: lo más
probable es que ocurra lo más probable.

Nada muy profundo, como decíamos, pero que nos descubre que la entropía
tiene una cerrada relación con la probabilidad. Nos queda por investigar
qué tiene que ver la entropía con la información.


\Section El demonio de Maxwell

Podemos describir una secuencia de un millón de unos con unas pocas
palabras. Es lo que acabo de hacer. En cambio, para describir una
secuencia completamente desordenada de unos y ceros, lo mejor que
podemos hacer es escribir la secuencia completa, ya que no hay forma
alguna de expresarla más concisamente. Significa esto que una secuencia
muy ^ordenada^ contiene muy poca ^información^, y una ^desordenada^
mucha.

Para la ciencia, la información es exactamente ^negaentropía^, o sea,
entropía simplemente cambiada de signo, porque así la definió
\[Shannon], que fue el ingeniero estadounidense que fundó a mediados del
siglo {\sc xx} las ^telecomunicaciones^. Bueno, porque así la definió, y
porque la teoría resultante ha resultado ser muy útil. Mejor, porque así
la definió, porque es útil, y porque explica una famosa ^paradoja^
enunciada por \[Maxwell], que fue el físico escocés del siglo {\sc xix}
que descubrió las leyes de la ^luz^, y dedujo de ellas que la luz es una
onda electromagnética.

La situación propuesta por \[Maxwell] comienza con un ^gas^ contenido en
un recipiente con dos compartimentos, digamos a la izquierda y a la
derecha, y una portezuela comunicándolos. El gas de los dos
compartimentos está inicialmente a la misma temperatura y, por lo tanto,
no se puede aprovechar para que realice trabajo. La cuestión es que si
un ^demonio^ controlara la apertura y el cierre de la portezuela, podría
conseguir que la temperatura de uno de los compartimentos fuera mayor
que la del otro, y entonces sí que podría obtenerse trabajo. Por si no
lo recuerda, la ^temperatura^ de un gas mide la velocidad media de sus
moléculas. Es decir, el demonio tendría que observar la velocidad de
cada molécula que se acercara a la portezuela, para determinar si debía
abrirla o no. De este modo, el demonio podría disminuir la ^entropía^
del sistema, esto es, podría ^ordenarlo^, por ejemplo dejando pasar al
lado izquierdo las moléculas más rápidas, y al derecho las más lentas.
Al final el compartimento izquierdo estaría a una temperatura mayor que
el derecho, y podría obtenerse trabajo del sistema.

El demonio de \[Maxwell] disminuye la entropía del sistema merced a la
información que usa para controlar la portezuela, pero resulta que el
gasto entrópico (en forma de aumento de entropía) que le supone al
demonio obtener dicha información es justo el mismo que finalmente gana
(en forma de disminución de entropía). Parafraseando a \[Heráclito] el
oscuro, que tuvo la suerte de vivir en una economía sin inflación ni
deflación, la entropía se convierte en información y la información en
entropía, al igual que las mercancías se cambian por oro y el oro por
mercancías.


\Section Shannon

Según la teoría matemática de la comunicación de \[Shannon], la cantidad
de ^información^ transmitida por un mensaje coincide con la cantidad de
^incertidumbre^ que despeja, y para medirla es necesario calcular la
variación de la ^probabilidad^ que supone. Se lo explicaré mejor con un
ejemplo.

Supongamos que yo no tengo ninguna información al respecto, que vivo en
^La Coruña^, donde la probabilidad de que esté ^lloviendo^ es del 50\%,
y que mi abogada \[Piripili], en quien confío absolutamente, me dice que
`está lloviendo'. Pues, en estas circunstancias, el mensaje de
\[Piripili] me aporta exactamente 1~^bitio^ de información, ya que para
mi la probabilidad de que esté lloviendo ha pasado de ser \frac(1/2),
que es la situación de máxima incertidumbre binaria, a la ^certeza^, o
sea, a probabilidad~1 (100\% en porcentaje). Para evitar que me lluevan
las críticas procedentes de la oficina de ^turismo^ de La Coruña, he de
hacer notar que se trata de una suposición, y tengo que confesar que la
probabilidad la he elegido para que el resultado fuera la unidad, por lo
que cualquier similitud con la realidad es pura coincidencia.

Por supuesto, a usted, que ya sabía que estaba lloviendo antes de oir a
\[Piripili], el mensaje `está lloviendo' no le ha proporcionado ninguna
información, 0~bitios, sobre el estado atmosférico actual. Porque para
usted la probabilidad no ha cambiado; era seguro (probabilidad~1) que
estaba lloviendo y sigue siéndolo (probabilidad~1). En cambio, si, por
ejemplo, consideraba mentirosa a \[Piripili], el mensaje sí que le ha
proporcionado información sobre ella. Para evitar las críticas de
\[Piripili], he de hacer notar que se trata de una suposición que tiene
una intención exclusivamente didáctica. ¡Qué pesada se hace la
^escritura defensiva^!

Ya tiene usted argumentos para explicar por qué no es ^noticia^ que un
^perro^ muerda a un ^niño^, pero sí que lo es que un niño muerda a un
perro. Cuanto más improbable es un acontecimiento, más noticioso, porque
su noticia contiene más información. Esta explicación también se aplica
a las ^sorpresas^. Una interpretación adulta simplista, asume que los
niños pequeños han de estar constantemente sorprendidos ante las
maravillas del mundo, porque para ellos todo es nuevo. Pero es justo al
revés. Como todo les resulta ^nuevo^, lo que sorprende a los niños
pequeños es lo que no es nuevo, lo que se repite. A los niños les
sorprende lo que a los adultos nos aburre.


\Section El límite

``En el marco topológico radical el ^límite^ es {\em sólo límite}; es
límite absoluto y reflexivo; es límite {\em puro}, límite de sí a sí,
{\em límite de lo mismo}: es el límite {\em en tanto que límite}. La
topología dice lo que es el límite {\em como} límite, límite interior y
reflexivo, referido a sí, a su propia mismidad. Piensa el límite como
{\em diferencia en tanto que diferencia} interna a la {\em mismidad en
tanto que mismidad}.''

La ^información^ contenida en la cita anterior, en la que he respetado
hasta las cursivas por si facilitan su entendimiento, depende del
^conocimiento^ previo. Es claro que para \[Trías], que la escribió,
encierra información. Para otro puede contener cero bitios; para usted,
no lo sé, y para mi, tampoco.
% \[Trías] \(Los límites del mundo, 2ª) página 358:

Lo que nos importa es que la cantidad de información depende de las
teorías, las creencias y los conocimientos previos del sujeto que recibe
la noticia. La otra conclusión es que la información está relacionada
estrechamente con la ^probabilidad^. El universo es información, y la
información mide cambios de probabilidades. Así que ahora tenemos que
averiguar qué es la probabilidad.


\Section La probabilidad

La ^probabilidad^ sirve para estudiar aquellas situaciones en las que
desconocemos lo que va a ocurrir, pero sabemos cómo tiene que ser la
^estadística^ de lo que suceda. Si lo desconocemos todo, entonces ni la
probabilidad nos vale. Pero, ¿hay alguna situación sobre la que no
podamos hacer ninguna conjetura? Y aun si la hubiera, ¿sería
provechoso elucubrar sobre tan ignoto asunto?

La probabilidad sirve, por ejemplo, para investigar el juego de ^dados^,
en el que no sabemos si esta vez saldrá o no un as, pero sí que sabemos
que en promedio sale una vez de cada seis. Más concretamente, la
probabilidad de un determinado evento coincide con la ^frecuencia^ de
ese evento, cuando la serie es lo suficientemente larga. Así, en el caso
del dado, la probabilidad de que salga un as coincide con la frecuencia
de obtener un as, que es de un sexto,
\frac(1/6), porque, como dijimos, sale en promedio una vez de cada seis.

Cuando no tenemos una teoría para predecir qué pasará, siempre podemos
valernos de la estadística de lo que ha sucedido en ocasiones
semejantes. No sabemos, por ejemplo, si dentro de justamente un año
lloverá aquí en ^La Coruña^, pero podemos emplear la estadística
pluviométrica del mes en curso para hacer una ^predicción^ más o menos
fiable.

También puedo utilizar una estadística relativa al lugar en el que ha
disfrutado usted sus ^vacaciones^ estos últimos años, para vaticinar
cuál será su destino el próximo verano. Si, por ejemplo, y que no se me
moleste nadie, observo que en los quince últimos años usted ha preferido
la ^playa^ doce veces, frente a las tres que eligió la ^montaña^,
entonces puedo atreverme a pronosticar que irá a la playa con cierta
garantía de acertar. Por supuesto, para usted todo esto no tiene ningún
valor, ya que sabe cuales fueron exactamente las razones por las que
viajó cada año al lugar elegido. Tal vez un año fue a causa de una
oferta muy económica que no pudo rechazar, quizás otra vez porque se lo
propuso un familiar al que no veía durante el resto del año, y puede que
en otra ocasión cediese ante la insistencia de sus hijos; en fin, cada
año sopesó las posibilidades que se le ofrecían y eligió racionalmente
la que le pareció más ilusionante.

Estos ejemplos me sirven para mostrar que es suficiente disponer de las
frecuencias relativas de los distintos eventos para poder utilizar las
probabilidades. O, en negativo, que no es preciso conocer las ^causas^
de los eventos para sacar provecho de las probabilidades.


\Section Haciendo estadísticas

Mientras estamos elaborando la ^estadística^ de un determinado fenómeno,
ocurre necesariamente que cada nuevo dato modifica la ^frecuencia^ de
todos los eventos, excepto en un caso. Fíjese que la frecuencia de un
determinado valor es el cociente de dividir el número de veces en que el
^dato^ ha tomado dicho valor entre el número total de medidas
realizadas. Por ejemplo, la frecuencia con la que usted veranea en la
playa es de \frac(12/15), porque ha ido doce de las quince veces a la
playa.

De modo que cada nueva ^medición^ suma necesariamente uno al total de
medidas, o sea, aumenta el denominador en uno, mientras que suma uno
únicamente al numerador del valor que ha ocurrido. Esto significa que
disminuye la frecuencia de todos los valores, salvo la del que ha
ocurrido esta vez, que aumenta. Por ejemplo, si este año va usted a la
montaña, entonces su frecuencia de veraneo en la playa pasará a ser
\frac(12/16), que es menor que \frac(12/15), mientras que su frecuencia
de veraneo en la montaña aumentará de \frac(3/15) a \frac(4/16).

Si le gustan las ^matemáticas^ puede entretenerse demostrando el
siguiente teorema, que es el que viene al caso.
$$0<n<m :\quad {n \over m+1} < {n \over m} < {n+1 \over m+1}$$

Hay una excepción, que ocurre si en todas las mediciones, desde la
primera, se ha obtenido siempre el mismo valor del dato, porque entonces
la frecuencia del valor que ocurre siempre es uno, y no crece, y la
frecuencia de los demás valores, que nunca han ocurrido es cero, y no
disminuye. En términos del teorema matemático, estamos en los casos
extremos, $n=m$ y $n=0$.

Si siempre ocurre lo mismo, como en la excepción, decimos que la
situación es ^determinística^, y si no siempre sucede igual, decimos que
es ^probabilística^.


\Section Yo era una probabilidad

Un estudiante decidió echar a suertes su plan para la tarde previa a un
examen definitivo. Tomó una ^moneda^ y se prometió que, si tocaba cara,
se daría una vuelta en bicicleta, si salía cruz se iría al cine, y si
caía de canto, se quedaría a estudiar. ¡Le gustaba tener en cuenta todas
las posibilidades!

Hay que decir que la moneda de un peso landés, de curso legal allí
entonces, era un cilindro largo, apodado palo, que siempre caía
acostado, pero que el estudiante buscó en su monedero hasta encontrar
una moneda de medio peso. La moneda de medio peso era una moneda más
normal, en forma de disco, de algo menos de dos centímetros de diámetro.

El caso es que lanzó la moneda y salió cruz. Pero como no le convenció
ninguna de las películas programadas en los cines de la ciudad, y además
el día era soleado y primaveral, decidió ir a dar una vuelta en
bicicleta. No obstante, antes de comenzar su excursión, tenía que ir
andando a recoger la bicicleta al garaje de sus tíos, que era el lugar
en donde estaba guardada.

Pero, justo al salir del portal de su casa, se tropezó con una
compañera, que le gustaba mucho, pero con la que apenas había tenido
oportunidades de conversar. El estudiante aprovechó el encuentro para
invitarla a un helado. Ella aceptó, y él no fue aquella tarde a pasear
en bicicleta. Cinco años después tuvieron su primer hijo. Ese primer
hijo es tu padre ---finalizaba sentenciando mi ^abuelo^ siempre que me
contaba cómo conoció a mi abuela, y a mi siempre me recorría la espalda
un escalofrío al pensar que, cuando lanzó aquella moneda, yo sólo era
una ^probabilidad^.


\Section Las fuentes de la probabilidad

Siento que tengamos que dejarnos de cuentos para dedicarnos a considerar
con seriedad el papel de la ^probabilidad^ en la ^ciencia^. Pero antes,
déjeme que le dé un consejo: si usted quiere llegar a pasar por
^filósofo^, no incluya nunca historias ligeras y ejemplos sencillos en
sus escritos. En fin, no haga como yo hago, sino como yo le digo que
haga. Sigamos.

Si conocemos las causas de un determinado fenómeno, entonces podemos
formular la ^ley^ causal correspondiente. Cuando no las conocemos, no
podemos enunciar una ^ley causal^, pero sí que podemos establecer una
^ley probabilística^.

Si un fenómeno es aleatorio, entonces sólo puede ser descrito
científicamente por una ley probabilística. Cuando desconocemos parte de
las circunstancias que concurren en un suceso, estamos igualmente
obligados a utilizar una ley probabilística para describirlo
científicamente. Por último, si tuviéramos que describir científicamente
el comportamiento de un sujeto libre, también tendríamos que usar una
ley probabilística.
$$\hbox{Indeterminación}\llave{Azar\cr Ignorancia\cr Libertad}$$

La ley del ^dado^ se debería al ^azar^, o al ^desconocimiento^ de todas
las variables físicas que intervienen en una tirada de dado. La ley que
describe el destino de sus ^vacaciones^ se debería a su ^libre
albedrío^, o a la ^ignorancia^ que supone no conocer todas las
circunstancias y procesos cerebrales que intervienen en sus decisiones.

Cuando nos encontramos con indeterminación, cualquiera que sea su
origen, hay ^incertidumbre^ y no podemos asegurar qué sucederá, así que
lo más que podemos hacer es presentar la ^estadística^ de lo que ha
ocurrido en ocasiones semejantes, es decir, tenemos que usar
^probabilidades^, o sea, tenemos que contar y calcular ^frecuencias^.


\Section Los principios de racionalidad

Uno de los principios básicos de la ^ciencia^ es el ^principio de
legalidad total^ (en inglés \latin{lawfulness}), que establece que las
^leyes^ de la naturaleza rigen todo cuanto ocurre. Además, otro
principio científico auxiliar del anterior, el ^principio de
inteligibilidad^, garantiza que es posible descubrir tales leyes.

Estos dos principios aseguran, el primero, que la naturaleza es racional
y, el segundo, que es racional la investigación científica. Afirman que
la ciencia puede explicarlo todo. Que algo no tenga una ^explicación^
científica significa, si estos dos principios se sostienen, que la
ciencia todavía no ha dado con la explicación. Según estos principios de
racionalidad, el ámbito de estudio de la ciencia es la naturaleza
completa, sin excepciones.

Además de encomiables, los principios de racionalidad se han mostrado
muy eficaces. Porque aceptar como explicación un ^milagro^ es lo mismo
que no explicar, es dar por buena la ^ignorancia^ y no perseverar en la
búsqueda del ^conocimiento^. Rechazar de plano los prodigios y los
milagros es la única manera de intentar entender lo que ocurre.

Es importante percatarse de que las leyes a las que se refieren estos
principios han de ser ^leyes causales^ no probabilísticas. Porque, como
hemos visto, se podrían hacer estadísticas de los prodigios y de los
milagros, y de las decisiones tomadas libremente. Es decir, que si las
leyes a las que se refieren los dos principios de racionalidad fueran
^leyes probabilísticas^, entonces afirmarían trivialmente que pueden
hacerse estadísticas de cualquier fenómeno; ya sabemos que basta contar
y dividir para calcular las frecuencias.

Puesto que la irracionalidad cabe en las estadísticas, las verdaderas
leyes son las leyes causales. Las leyes causales explican, son
racionales. Las leyes probabilísticas no explican sino que, al
contrario, se utilizan cuando se desconocen las causas. Las leyes
probabilísticas no son más que estadísticas.

Por ser la ^libertad^ una de las fuentes de la probabilidad, nuestra
intención de incorporarla a la ^ciencia^ choca contra el principio de
legalidad total. Y es que en el principio de legalidad total no queda
espacio para el libre albedrío, que es, precisamente, una zona libre de
leyes causales. Así que, para salvar la libertad tendremos que rebatir
tal principio. Afortunadamente para nuestros propósitos, que el
principio de legalidad total sea deseable, no prueba que sea verdadero.


\Section Bohr contra Einstein

La ^probabilidad^ apareció por primera vez en la física durante el siglo
{\sc xix} al estudiar el calor y los gases, como vimos al hablar de la
^entropía^. Se aceptó que cuando el número de partículas es muy grande,
no resulta práctico aplicar las leyes de la ^mecánica^ newtoniana a cada
una, y agregar los efectos obtenidos para obtener el estado global del
sistema. En el caso de un ^gas^, es fácil que tengamos billones de
billones de moléculas, literalmente unas $10^{24}$ partículas, por lo
que lo único prácticamente posible es utilizar estadísticas y cálculos
probabilísticos.

La segunda aparición, a principios del siglo {\sc xx}, fue mucho más
controvertida. Según la ^física cuántica^, las partículas subatómicas
individuales, como el electrón o el fotón, tienen un comportamiento
probabilístico. Recuerde que la física cuántica trata de lo más pequeño,
y que si las moléculas de los gases están compuestas de átomos, de lo
que estamos ahora hablando es de los componentes de los propios átomos.

Que sean probabilísticas las leyes que rigen la naturaleza en sus más
finos detalles, como nos dice la física cuántica, no gustó a todos los
científicos. \[Einstein] fue posiblemente el menos convencido. Su
postura la resume una de sus más célebres frases: ``^Dios^ no juega a
los ^dados^''. Lo que sin duda molestaba más a \[Einstein] del
probabilismo cuántico era que contrariaba se fe en la completa
racionalidad del universo. Admitir que, en la misma base legal del
universo, impera la probabilidad es admitir la derrota de la razón.
Porque, como vimos, la ley probabilística es una ley de segunda clase
que prescinde de las causas y atiende únicamente a las estadísticas, por
lo que sólo se aplica, como mal menor, cuando no puede aplicarse la ley
causal, que es la verdadera ley.

Su principal antagonista en esta discusión fue \[Bohr], el físico danés
de la primera mitad del siglo {\sc xx} que descubrió la estructura del
^átomo^ de hidrógeno. La posición de \[Bohr] era, a la vez, pragmática y
^escéptica^. En el apartado práctico, afirmó que la teoría cuántica es
válida porque permite predecir lo que ocurrirá con una precisión que
ninguna otra teoría es capaz de alcanzar; y esto sigue siendo cierto.
Era escéptico porque pensaba que posiblemente nuestra mente no estaba
diseñada para entender el funcionamiento de unos objetos tan alejados de
nuestra experiencia cotidiana como las partículas subatómicas.


\Section La extrañeza cuántica

El ^escepticismo^, y toda la disputa entre \[Einstein] y \[Bohr], hay
que verlo en medio de la extrañeza general que produce intentar entender
el funcionamiento de las partículas subatómicas según la descripción que
proporciona la ^física cuántica^. Los mejores exponentes de la insólita
naturaleza de las teorías cuánticas son las ^paradojas cuánticas^.

No voy a extenderme con este asunto. Ya le dije que para ser ^filósofo^
necesita tener conocimientos científicos, y que éstos no se improvisan.
De modo que, si usted sabe física cuántica, le bastarán mis
explicaciones y, si no, tendrá que evitar esta rama de la filosofía.
Sepa que esto no es grave, porque es tal el actual grado de
especialización de la ^ciencia^, que ningún ^científico^ tiene un
conocimiento, siquiera somero, de todas las disciplinas científicas. Y
si un científico no lo tiene, menos lo tiene que tener un filósofo.

Para las teorías cuánticas, mientras no se ^mide^, el estado de una
partícula cuántica es una onda de probabilidades que evoluciona de
acuerdo a la ^ecuación de \[Schrödinger]^. En cambio, al medir, se
obtiene siempre un valor determinado.

La primera impresión es que al medir se provoca el colapso de la onda de
probabilidades, que se condensa en un único valor determinado. Una vez
realizada la medida, y tomando como punto de partida el valor medido, y
no los otros estados que también tenían alguna probabilidad antes de
efectuar la medición, se vuelve a desarrollar la onda de probabilidades
del modo descrito por la ecuación de \[Schrödinger]. Repito, parece que
al medir obligamos a la partícula a decidirse por un estado concreto, y
que, en cambio, mientras no medimos, la partícula simplemente no toma
ninguna decisión y mantiene abiertas todas las ^posibilidades^.

Para \[Einstein] es ridículo pensar que la naturaleza juega al
^escondite inglés^ con el ^sujeto^ y que, como en el juego, se queda
quieta cuando la miran, y se mueve en cuanto el sujeto tuerce la cara y
mira para otro lado. Para \[Bohr] el comportamiento de las partículas
cuánticas es efectivamente ^aleatorio^, y haciendo los cálculos al modo
de la teoría cuántica se acierta a precedir lo que ocurrirá, así que, si
no podemos entender por qué es así, es mejor no darle más vueltas.


\Section La teoría de la medida

\[Bohr] pensaba que los fenómenos ^cuánticos^ son ^azarosos^, pero usted
y yo sabemos que la probabilidad puede tener otros orígenes. Así que
también se exploró la posibilidad de que el comportamiento real de las
partículas subatómicas fuera ^desconocido^, sospechando de la medida,
sin duda a causa de su peculiar interpretación según las teorías
cuánticas.

Para establecer una ^teoría de la medida^ basta enunciar dos principios
que parecen verificarse en cualquier medición: el ^principio de
determinación^ y el ^principio de interacción^.

Principio de determinación: en toda ^medición^ se obtiene un valor
determinado, que llamamos ^dato^ o ^medida^. La única manera de obtener
medidas es midiendo y, por lo tanto, no tenemos dato alguno de lo que
ocurre entre dos mediciones.

Principio de interacción: el dato se obtiene al interaccionar el aparato
de medida con el fenómeno a medir. Dicho de otra manera: toda medición
^perturba^ el fenómeno. Los acontecimientos probablemente serían
diferentes si no se midiera.


\Section La aporía de la medida

Aquí hay una dificultad que se puede soslayar de dos maneras. Una
consiste en despreciar la ^perturbación^, y de aquí que sólo el
advenimiento de la mecánica cuántica, al tratar de lo más pequeño, la
haya desvelado. Pero una vez conocida, es nocivo ignorarla. Veamos,
pues, en detalle, la ^aporía^ que plantea la medida.

Sólo podemos obtener datos midiendo, de modo que las leyes sólo pueden
ser verificadas si aciertan sus predicciones sobre el resultado de las
mediciones, o sea, si adelantan los ^datos^. En estas circunstancias, la
mejor ley que podemos descubrir es aquélla capaz de predecir con toda
exactitud el resultado de las mediciones. Tal ley, con ser la mejor,
nada nos dice de lo que ocurriría si no midiésemos el fenómeno. O tal
vez sí, pero sólo tal vez.

Por lo tanto, dado que estamos condenados a ^desconocer^ lo que ocurre
entre mediciones, las leyes de los fenómenos no perturbados han de ser
_probabilísticas<desconocimiento>. Observe usted que esto es
estrictamente aplicable a cualquier teoría, y que solamente puede
despreciarse si se desprecia la perturbación que, según el ^principio de
interacción^, toda ^medición^ causa al fenómeno.

La palabra `^medida^' es una palabra técnica que parece reducir el
efecto de la aporía encontrada al dominio de la ciencia. Pero no se 
confíe, porque no es así, ya que cualquier toma de datos es una
medición. Lo que hacen nuestros ^sentidos^, como la vista o el oído,
son mediciones.
$$\hbox{Teoría de la sensación} = \hbox{Teoría de la medida}$$


\Section La regañina

Si ha permanecido usted atento a pesar de la perplejidad de mis
explicaciones sumarísimas de la ^física cuántica^, estará revolviéndose
contra mi en su butaca. Si no es ésta su disposición de ánimo actual,
entonces, o está leyendo con demasiada ligereza, o ha sucumbido al
sueño, y, en cualquiera de los dos casos, debería dejar inmediatamente
la lectura, para retomarla cuando se encuentre más tranquilo y
despierto, retrocediendo, además, unas cuantas páginas, porque no se
está enterando de las últimas secciones que ha leído.

\breakif4

Perdone la ^regañina^, que seguramente no se merece porque, aunque es
cierto que todo este embrollo de la aporía de la medida es
descaradamente objetivista, y de nada le vale a un subjetivista como yo,
también es verdad que, para descubrirlo, ayuda mucho tener manía al
^objetivismo^. Y, las paranoias nunca son convenientes. Por ejemplo, en
mi propio caso, que he sido tomado por tal obsesión, me veo obligado a
dudar a cada paso de mis conclusiones, porque sospecho que son ilusiones
sin fundamento producidas exclusivamente por unas ganas inmensas de
refutar el objetivismo que me poseen sin remedio.

Después de esta confesión me parece que le asaltan a usted las dudas.
Eso está bien. Recapitulemos. \[Descartes] estaba en lo cierto: lo único
seguro es que estoy pensando. \[Kant] tenía razón: lo que está fuera no
puede ser pensado. Pero, a pesar de ello, nuestra relación con el
entorno ^exterior^ es exitosa, incluso demasiado exitosa, y fructífera.
Por lo tanto nuestra interpretación objetivista tiene que ser lo
suficientemente acertada y, además, no tenemos otra. Así que ---se
pregunta usted--- ¿para qué darle más vueltas? Supongamos, como
hipótesis, que las cosas son como las pensamos.

No seré yo quien le contradiga. Yo mismo, a efectos prácticos, supongo
que las cosas son como las veo y las pienso. Pero sé que no son así. Y
si a usted no le vale, como razón suficiente para acometer estas
extravagantes pesquisas, la búsqueda de la verdad por ella misma, ni le
sorprende que, tanto el nexo entre una persona y su entorno, como la
conexión entre un ser vivo y el resto de la ^vida^, ambos resulten ser
interfaces que transfieren ^información^, ni le parece curioso que la
información sea convertible en ^entropía^, y por consiguiente en trabajo
físico, y que la información tenga una cerrada relación con la
^probabilidad^, probabilidad que reaparece en la misma base cuántica de
la física causando una enorme perplejidad a alguien tan perspicaz como
\[Einstein], si nada de esto le intriga, entonces le recuerdo que aún ha
de conciliar su creencia en la ^libertad^ y el libre albedrío con el
legalismo excluyente de la ciencia materialista.

Otra cosa es cierta, sin embargo. El ^saber^ no da la ^felicidad^, y
frecuentemente ocurre justo lo contrario. Este es, sin duda alguna, el
punto crucial: el ^filósofo^ no se contenta con ser feliz. Es filósofo
el que quiere saber. De manera que si usted no siente ^curiosidad^ por
estos asuntos, debe dejar definitivamente de leer este ^libro^ aquí
mismo, ya.


\Section Heráclito contra Parménides

Le decía que la aporía de la medida es objetivista. La aporía dice que
las leyes de los fenómenos no perturbados, esto es, no medidos, han de
ser ^leyes probabilísticas^ porque nunca podremos
_conocer<desconocimiento> lo que ocurre entre dos mediciones. Ocurre
entonces con el fenómeno no perturbado lo mismo que sucedía con el
\latin{^Noumenon^} kantiano. En cuanto queremos
conocer el fenómeno no perturbado hemos de medirlo, y, al medirlo, lo
perturbamos y deja de ser un fenómeno no perturbado. Pero ya hemos
aprendido que cuando nos parece que hablamos del \latin{Noumenon},
estamos, más bien, hablando del objeto. Veámoslo.

El ^objeto^ es un artefacto de la ^percepción^ que simplifica los datos
captados por los sentidos. En el caso de la visión, los datos son los
puntos de color que forman la imagen retinal, y el objeto es una región
a la que adscribimos unas pocas propiedades. Esa es la manera espacial
de comprimir la ^información^ que hemos investigado, pero hay otro modo
de ^compresión^ que es temporal. Si dos imágenes retinales temporalmente
consecutivas son parecidas, porque muchos puntos de color mantienen sus
valores, o cambian poco, entonces podemos ahorrarnos rehacer el dibujo,
ya que nos vale el anterior con, si acaso, alguna modificación parcial.
Así que los objetos perduran por la misma razón por la que hay objetos,
por ^economía^ cognitiva.

De modo que \[Heráclito], al decir que ``todo ^cambia^'', se refería a
la imagen retinal. Pero lo que nosotros vemos, una vez que los datos
captados en la retina son procesados y resumidos por los procesos de la
percepción, son objetos que perduran, aunque vayan cambiando sus
propiedades. Por eso, para el objetivista arquetípico que fue su
antagonista \[Parménides], la realidad fundamental ^permanece^
inalterada.

El objeto resume ^espacio^ y ^tiempo^.


\Section La medida es el fenómeno

¿De dónde viene, pues, la idea del fenómeno no perturbado? Del
^objetivismo^, que interpreta erróneamente que lo que perdura es el
^objeto^ exterior. Así que el objetivismo postula que el objeto exterior
causa los datos retinales, y que el objeto exterior permanece. Dadas
estas dos condiciones, se deduce que el objeto exterior también genera
datos cuando no lo miramos. El fenómeno no perturbado es, para el
objetivismo, el fenómeno generado por el objeto exterior cuando no lo
observamos.

Afirmar la existencia exterior del objeto es conferir a éste una
^substancia^ inasequible que lo sostiene entre mediciones. Negar la
existencia del objeto exterior es aceptar prudentemente que todo nuestro
^conocimiento^ proviene de las medidas. Todo nuestro conocimiento
proviene de las medidas.

Una vez que somos capaces de prescindir de la existencia del objeto
exterior, se queda sin sentido el concepto de fenómeno no perturbado.
Porque, si medir es la única manera en que experimentamos los fenómenos,
un fenómeno no perturbado sería un fenómeno no manifestado, lo que es
una contradicción de términos. Radicalmente: hay que eliminar el
^principio de interacción^ de la teoría de la medida. La ^medida^ es el
fenómeno. Y, por si no está claro, se lo repito: el fenómeno es la
medida. Así, ya no hay aporía.

Cierto, hemos liquidado la aporía de la medida, pero seguimos sin
explicar las ^paradojas cuánticas^.


\Section La solución cuántica

Cuando \[Newton] se cuestionó la obviedad de que las manzanas se caen
cuando maduran, estaba tratando con objetos típicos. Una manzana o una
bala de cañón son objetos que vemos naturalmente sin ayuda de aparatos y
que podemos manipular directamente. La mecánica newtoniana codifica
^información^, pero lo hace respetando la simplificación objetiva del
exterior.

No es éste el caso de los objetos cuánticos. La física cuántica describe
lo que les sucede a las cosas más pequeñas. Como era de esperar, ya que
los defectos del ^objetivismo^ se evidencian al acercarnos, hay
^grietas^ en esta parte del edificio. Y, en este caso, mi opinión no
difiere de la opinión común, porque la propia ciencia materialista
reconoce la existencia de las paradojas cuánticas.

Para disolver las ^paradojas cuánticas^ basta con cambiar la
interpretación objetivista por la subjetivista. Para la interpretación
objetivista, al medir se colapsa, como vimos, la ^onda de
probabilidades^ y, como consecuencia, cambia el estado de la partícula
cuántica que hay ahí fuera. Para la ^interpretación subjetivista^, que
no cree en la existencia de objetos cuánticos ahí fuera, al medir lo que
cambia es el estado de conocimiento acerca de lo que ocurre ahí fuera
que tiene el sujeto que está midiendo, de modo que el colapso cuántico
es la transición que va de no conocer a conocer. Antes de medir lo que
tenemos son varias posibilidades, una por cada medida posible. La
^medición^, en vez de causar mágicamente el desenredo (en inglés
\latin{disentanglement}) del estado de una partícula cuántica que
supuestamente existe fuera, lo que hace es cambiar el estado de
^conocimiento^ de quien mide. Y esto último es lo que describen las
^leyes cuánticas^. Obvio, y no me diga que no lo es.

La interpretación subjetivista disuelve tanto la paradoja del ^gato^ de
\[Schrödinger] como la de  \[Einstein]-\[Podolsky]-\[Rosen]. Esta última
paradoja, y también el famoso experimento de las dos ranuras (en inglés
\latin{the two slits experiment}), muestran, además, que las propiedades
no están localizadas, es decir, nos enseñan que no existen los objetos.
Demuestran que la simplificación objetivista tiene sus limitaciones y
que, seguramente, el ^espacio^ y el ^tiempo^ no son como nos los
imaginamos.


\Section La ignorancia

Hemos prescindido del ^principio de interacción^, pero no del ^principio
de determinación^ de la medida. Las medidas no son nunca aleatorias,
porque cuando medimos obtenemos datos determinados. La ^medida^ es el
fenómeno, \latin{ergo} los fenómenos no pueden ser aleatorios. Esto
elimina al ^azar^ como fuente de ^indeterminación^ o ^incertidumbre^.
No hay azar, sino ^ignorancia^, o ^desconocimiento^, y ^libertad^.
$$\hbox{Indeterminación}\llave{Ignorancia\cr Libertad}$$

La ^vida^ se juega en la ^incertidumbre^, entre la ignorancia y la
libertad, y su fruto es el ^conocimiento^. Por supuesto, la vida juega a
no morir. La ignorancia no se puede deber a la imposibilidad de conocer
el fenómeno no perturbado, porque negamos que haya tal. Lo he escrito en
plural porque supongo que está usted conmigo, ¿no es así? Las causas de
la ignorancia son varias.

Podemos no conocer parte del ^presente^ por falta de atención (medición
no anotada), o por carecer de acceso a parte del presente (medición no
realizada), o por incapacidad para asimilar toda la información que se
nos presenta (capacidad insuficiente del canal de datos).

El desconocimiento del ^pasado^ puede deberse a que el dato no quedó
retenido en memoria alguna (medida no registrada), posiblemente a causa
de la finitud de las memorias, o al olvido (registro borrado o
alterado), o a que falla la recuperación del recuerdo, aunque esté
retenido (registro inaccesible).

El ^futuro^ ni ha sido medido ya, ni puede estar siendo medido ahora.
Por esta razón, es mucho más lo que ignoramos del futuro que lo que
conjeturamos que pasará.

Del pasado hay medidas realizadas y en el presente se están realizando
las mediciones, pero ni hay medidas realizadas del futuro, ni en el
futuro se están realizando mediciones. Las causas de la ignorancia son
varias, pero hay diferencias si la ignorancia se refiere al pasado, al
presente, o al futuro.


\Section El tiempo

Todo nuestro conocimiento proviene de las medidas, y la ^medición^
distingue el futuro del pasado.

Sin embargo, para las teorías físicas que fundamentan toda la ^ciencia^
actual, como la teoría de la relatividad y la teoría cuántica, no hay
diferencia entre el futuro y el pasado. Concretamente, en las ecuaciones
de estas teorías se puede sustituir la variable que representa al
^tiempo^, $t$, por su opuesta, que es $-t$, y las ecuaciones no cambian,
$\mathop\Phi(t) \equiv \mathop\Phi(-t)$. Es decir, sus ecuaciones son
^simétricas^ respecto al tiempo. No distinguen el ^futuro^ del ^pasado^
ni el avance del retroceso en el tiempo. No ven la ^flecha del tiempo^,
no ven que el tiempo huye y nunca vuelve. Los libros de divulgación
utilizan como ejemplo que, para estas teorías fundamentales, no hace
diferencia que la película se vea avanzando normalmente desde el
principio hasta el final o que se vea retrocediendo desde el final hasta
el principio.

En estas teorías con ecuaciones simétricas respecto al tiempo, el tiempo
es reversible porque no distinguen ni dan preferencia ni significado
especial a ninguna dirección del tiempo y, en particular, no obligan a
que el tiempo crezca irreversiblemente. Por el contrario, ambas
posibilidades, crecimiento y decrecimiento del tiempo, son siempre
contempladas sin distingos y en igualdad de condiciones.

La medición distingue el pasado del futuro, y ella misma está dirigida
en el tiempo. La ^medida^ es el resultado de la medición. La medición
finaliza cuando se obtiene la medida, pero ha de comenzar antes, no
después. Se prepara la medición antes y después se obtiene la medida,
también llamada ^dato^. Por lo tanto, en una teoría que no distinga el
pasado del futuro no caben las mediciones. Pero, como usted y yo
sabemos, el tiempo no es así: el tiempo es irreversible.


\Section La muerte

Parece absurdo que la ^ciencia^, que concede la autoridad definitiva e
inapelable a los datos obtenidos en las ^mediciones^, proponga teorías
que demuestran la imposibilidad o el sinsentido de las mediciones. Y,
además, si el ^tiempo^ es irreversible, que lo es, entonces las teorías
físicas vigentes son fundamentalmente erróneas. Para obtener más
indicios sobre los motivos de este error, vamos a examinar otras
consecuencias de la ^simetría temporal^.

En primer lugar, un absurdo. Si el tiempo fuera reversible, yo podría ir
al pasado y matar a mi ^abuelo^ antes de que fuera a buscar su bicicleta
y conociera a mi abuela, pero entonces yo no habría nacido, y no podría
haber ido.

Otra consecuencia que tendría que el tiempo fuera reversible, y que el
^futuro^ fuera como el ^pasado^, sería que la ^libertad^ era imposible.
Porque la libertad concierne al futuro, y no al pasado, y la simetría
temporal imposibilita cualquier diferencia entre el pasado y el futuro.
Siendo el futuro como el pasado, y no habiendo libertad para cambiar el
pasado, tampoco la habría para intervenir en el futuro.
 $$\hbox{Tiempo reversible} \implies \hbox{No hay libertad}$$
Esta implicación nos asegura que, si las leyes científicas son
simétricas respecto al tiempo, entonces queda descartado que haya zonas
libres de ^leyes causales^, y, por consiguiente, queda asegurado el
cumplimiento del ^principio de legalidad total^.

Podemos dar la vuelta a la implicación y decir lo mismo, pero de otra
manera: si hay libertad, entonces el tiempo es irreversible.
 $$\hbox{Libertad} \implies \hbox{Tiempo irreversible}$$

Por último, y sin ánimo de ser exhaustivo, ni macabro, otra de las
consecuencias que tendría un tiempo reversible sería, obviamente, la
imposibilidad de la ^muerte^. Por esta razón no debe extrañarle que haya
un arraigado prejuicio en favor del tiempo reversible. Pero, superado el
prejuicio, esta consecuencia nos señala el fuerte vínculo que une a la
^vida^ con la libertad.


\Section La libertad

Un sujeto es libre si puede elegir como actuará. Un sujeto es libre
cuando puede elegir entre varios futuros posibles. La ^libertad^
concierne al ^futuro^, y no al ^pasado^.

Ignoramos lo que sucederá en el futuro a causa de la libertad. Porque,
si hay libertad, entonces el futuro no está determinado, sino abierto.
Y, por ese motivo, no es posible asegurar qué ocurrirá en el futuro. O
sea, que una causa suficiente de la ^ignorancia^ es la libertad.
 $$\hbox{Libertad} \implies \hbox{Ignorancia}$$

La libertad es una causa suficiente de la ignorancia que sólo concierne
al futuro. La libertad es suficiente para explicar que el futuro no es
como el pasado. La libertad es suficiente para explicar que es preciso
medir para obtener datos. Porque si hay libertad, y el futuro está
abierto y no cerrado como el pasado, entonces, antes de medir, siempre
habrá incertidumbre sobre el resultado de la ^medición^. Así explica la
libertad por qué medir siempre aporta ^información^. Y, en este caso,
siempre es siempre.

Cada medida disminuye la ignorancia. En positivo: cada medición que
realiza la vida aumenta el conocimiento que tiene la vida acerca de su
entorno. Tal progresión establece la ^flecha del tiempo^. Podemos
conocer lo ya medido, y por esto el pasado está determinado.
Desconocemos lo por medir, y por esto el futuro es incierto. El acto de
medir marca el ^presente^, que es la ^frontera^ entre el pasado y el
futuro. El ^tiempo^ es irreversible para la ^vida^ a causa de la
^medición^, que es necesaria a causa de la ^libertad^.

La vida juega a no ^morir^, y sin libertad no hay juego.


\Section La ley de la información incesante

La ^aleatoriedad^ es una propiedad del ^objeto^. La ^incertidumbre^ es
el estado natural del ^sujeto^. La ^libertad^ es la ^indeterminación^
referida al futuro. El ^desconocimiento^ es la incertidumbre referida al
pasado. La aleatoriedad es la proyección de la incertidumbre del sujeto
sobre el objeto. La ^probabilidad^ es una herramienta que usa el sujeto
para describir lo que él conoce, a pesar de su incertidumbre intrínseca.

Sostener que la ^ley^ física no describe el estado de la partícula, sino
nuestro ^conocimiento^ de la situación, disuelve las paradojas
cuánticas. Es obvio que cuando medimos conocemos lo que ocurre, para eso
medimos, y que cuando no medimos nuestro conocimiento es menos preciso,
porque somos ignorantes; usted y yo también.

Le propongo elevar a postulado la incertidumbre del sujeto, ¿se atreve?
Nunca ningún sujeto puede asegurar completamente cual será el resultado
de una ^medición^ aún no realizada. Porque hay libertad y el ^futuro^
está abierto, o porque el sujeto es ignorante, que tanto monta, monta
tanto. Y, como el sujeto no está seguro, el resultado de la medición
siempre le disipa alguna ^incertidumbre^ y le añade conocimiento. Ya
podemos formular la ^ley de la información incesante^: ``Toda medición
aporta ^información^ al sujeto que mide''.

La ley de la información incesante contradice la ^segunda ley de la
termodinámica^, que afirma justo lo contrario. Pero es que la ley de la
información incesante no se aplica a los sistemas cerrados, sino a la
^vida^, que es un ^sistema abierto^.


\Section La vida eterna

\[Kauffmann] busca desesperadamente el cuarto principio de la
termodinámica. Sabe que la ^vida^ no cabe en los tres principios
vigentes, y por esa razón piensa que se necesita un cuarto principio. En
mi opinión debería empezar de nuevo, es decir, debería olvidarse de la
^termodinámica^ y comenzar por el principio. O, mejor, sólo tiene que
hacerme caso. El nuevo principio que busca es la ^ley de la información
incesante^ que, en el supuesto de que el ^sujeto^ pueda acumular la
incesante información que recibe, puede reformularse como la ^ley de la
información creciente^: ``Toda ^medición^ incrementa la cantidad de
^información^ del sujeto''.

Otras formulaciones de la ley de la información incesante son: ``No es
posible asegurar completamente, antes de medir, el resultado de una
medición''; o, lo que es lo mismo: ``El ^futuro^ está abierto''. Y uno
de sus corolarios es: ``La ^muerte^ es inevitable'', ya que, al no poder
tener cubiertas todas las eventualidades (en inglés
\latin{possibilities}), el sujeto no puede evitar morir.

Así como los otros principios de la termodinámica se basan en
imposibilidades, como la imposibilidad de construir una ^máquina de
movimiento perpetuo^ (primer y segundo principios) y la imposibilidad de
alcanzar la completa quietud, o sea, la ^temperatura^ de cero absoluto
(tercer principio), este nuevo principio ---yo nunca lo pondría de
cuarto--- se basa en la imposibilidad de la ^vida eterna^. ¿Hay algo más
deseable y menos posible que la vida eterna? No me conteste, porque es
una pregunta retórica que sólo tiene el propósito de fortalecer los
fundamentos de la ley de la información incesante.

Estoy pensando que si ninguna cosa puede estar absolutamente quieta,
entonces todas las cosas son máquinas de movimiento perpetuo. Le dejo a
usted, como ejercicio, que evalúe la anterior _paradoja<aporía>. ¿Es
otra ^grieta^?


\Section La posibilidad

Acabo de escribir que la ^vida eterna^ es imposible, y antes que si hay
libertad, entonces hay varios futuros posibles. Y, si lo mira bien, las
probabilidades son meras posibilidades cuantificadas por sus
frecuencias. Pero, qué significa que algo es posible, o que no lo es, o
sea, ¿qué es la ^posibilidad^?

Para \[Aristóteles] la dicotomía máxima la conforman el ^acto^ y la
^potencia^. Lo actual es lo que es, y lo potencial es lo que puede ser.
\[Descartes] parte el mundo de otro modo: lo que ocupa espacio, lo que
está, es la \latin{res extensa}, la ^materia^, y lo otro, lo que no
está, es la \latin{res cogitans}, el ^espíritu^. En un modo de hablar
más moderno la misma dualidad se expresaría en los términos de
^realidad^ y ^teoría^.
$$\vbox{\halign{\strut \hss#:\quad&\hss#& \hss# $\cdot$ \hss& #\hss\crcr
 Es&         Acto& &    Potencia\cr
 Está&       Materia& & Espíritu\cr
 Se percibe& Realidad& &Teoría\cr
}}$$

El ^materialismo^ es el monismo material, que niega el espíritu. Parece
que nuestras observaciones, al devaluar el materialismo y, sobre todo,
al proclamar el panformismo del `todo es información', nos obligan a
adoptar el otro monismo, el monismo teórico, que niega la realidad. No
lo dé por hecho, porque `todo' incluye tanto la teoría como la realidad.
Además podemos generalizar la definición de vida propuesta por
\[Aristóteles]: `la ^información^ es potencia actualizable'. Pero no nos
distraigamos. Como decíamos, lo que ahora nos toca hacer es contestar
otra pregunta: ¿qué es la posibilidad?

La respuesta es fácil, o eso parece. Es potencia lo que no existe en la
actualidad. Es teoría todo cuanto no es real. Pero estas definiciones
fáciles son negativas, así que no aclaran lo que es la posibilidad, sino
lo que no es.


\Section El habla excede la realidad

Para superar la situación, le propongo examinar otra obviedad: podemos
hablar de lo que no existe, pero no podemos ver lo que no existe ---no
me diga nada, porque ya se lo advertí, es una obviedad. En general, la
^realidad^ es lo que se percibe a través de los sentidos, y la ^teoría^
es justo aquello que, no siendo real, puede ser dicho.

No podemos ver una ^pregunta^. Lo que vemos son las letras escritas, y
el signo de interrogación, porque sí que es posible decirla y
escribirla, pero no vemos la pregunta como vemos una ^piedra^, por sí
misma. No vemos los significados. No vemos las ^explicaciones^ ni los
^razonamientos^. La ^libertad^ no se ve. Tampoco se ve la ^vida^. El
cuerpo sí se ve, pero, como nos enseñó \[Aristóteles], no hay
diferencias visibles entre un cuerpo vivo y un ^cadaver^ reciente. De
todo esto que no vemos, estamos hablando. Podemos hablar de todo cuanto
vemos, y de más cosas. El ^habla^ excede la realidad.

Entendido así, el ^materialismo^ se limita a considerar una parte de
todo lo decible. `De lo que no se puede {\em ver} hay que callar',
hubiera concluido \[Wittgenstein] si hubiese sido materialista, que no
lo fue. O sea, que, si fuera coherente, el materialismo sólo hablaría de
la realidad. Para el propio materialismo esto es referirse a todos los
objetos que hay fuera, y al decir fuera quiere decir fuera del yo, así
que dentro no hay ninguno. Pero para nosotros ---o para mi, si es usted
un cobarde que no se atreve a unirse a mi--- la limitación materialista,
que consiste en interesarse únicamente por lo que la ^percepción^
construye, es una restricción injustificada y empobrecedora ---aunque le
advierto muy seriamente de que, si me deja solo, se quedará sin sus
laureles de gloria cuando finalmente triunfe el ^subjetivismo^.


\Section Una caricatura

Yo no sé qué le parece a usted, pero a mi me parece que vamos por el
buen camino. El nuevo edificio de la ^ciencia^ tiene que ser mayor que
el vigente, construido por los materialistas, porque no puede limitarse
a albergar la ^realidad^. Tienen que caber en él, además de todos los
actos, todas las posibilidades.

Es muy fácil hacer una ^caricatura^ de mi posición. Por cierto,
caricaturizar la propia situación es enormemente útil. La caricatura
exagera precisamente los rasgos más distintivos, los que ya en el
original son menos comunes. Es decir, la caricatura hace ^obvio^ lo
característico. Caricaturizar es señalar lo que distingue.

Imagínese ---es usted un caricaturista--- la ciencia que le estoy
proponiendo. Como es la ciencia de lo posible, de todo lo que puede ser
dicho, y como podemos decir que las manzanas no se caen, resulta que
\[Newton] se apresuró al establecer la ^ley de la gravedad^. Si se
hubiera esperado a la ciencia subjetiva, no le habría caido una
^manzana^ en la cabeza. Pero es que se tenía muy merecido el manzanazo,
el \[Newton] ese, por apuradillo.

Me parece que está usted muy ufano con su caricatura del subjetivismo,
pero hace mal en reirse de mi, sobre todo mientras está leyendo mi
^libro^. Porque yo tengo el control de lo que aquí se dice, y yo, como
todo el mundo, creo que la razón la tengo yo, y que son los demás los
que están equivocados, de modo que debería haber adivinado que, al
final, en mi libro triunfarán mis tesis. Y antes, también.

La ^caricatura^ no invalida el ^subjetivismo^, sino que destaca lo
obvio, que las manzanas caen. La manera de destacarlo es declarar que
podrían no caer. Sólo cuando alguien con la lucidez de \[Newton] es
capaz de percatarse de que las manzanas podrían no caer, se plantea la
necesidad de explicar por qué caen las manzanas.

La ^caricatura^ invalida el ^materialismo^, porque descubre que el
materialismo no tiene manera de explicar por qué hay que explicar la
caída de las manzanas. Es decir, el materialismo puede explicar por qué
caen las manzanas, pero, al prescindir de lo posible, se queda sin
argumentos que justifiquen la necesidad de explicar por qué caen las
manzanas. Dicho más rudamente, como para el materialismo las preguntas
no existen, porque no son reales, porque no se ven, sus respuestas son
respuestas a ninguna pregunta.


\Section Newton y Einstein

El ^subjetivismo^ considera todas las posibilidades, pero, si queremos
obtener provecho y evitar daños en nuestras interacciones con el
^exterior^, nos vale más asumir las ^leyes^ de la ^mecánica^ clásica de
\[Newton], y apartarnos de la trayectoria parabólica del proyectil, que
suponer ciertas las otras posibilidades.

Aunque hay otra ^posibilidad^ que también ha de ser estimada. Me refiero
a la mecánica de \[Einstein], que está contenida en su ^teoría de la
relatividad^ ---sí, esa que contiene la famosa fórmula $E=mc^2$. A lo
mejor, le han contado que la mecánica de \[Newton] es un caso particular
de la de \[Einstein], que se puede aplicar cuando las velocidades son
pequeñas en comparación con la velocidad de la ^luz^ ---que es la `$c$'
de la ecuación famosa. Si esto fuera verdad, entonces la mecánica
relativista sería una generalización de la clásica, pero no es así.

\[Kuhn] nos asegura que la ^masa^ relativista ---que es la `$m$' de la
fórmula--- y la masa clásica, aunque lleven el mismo nombre, no pueden
ser consideradas iguales, ni siquiera en reposo. Por lo tanto, ``la
teoría de \[Einstein] puede ser aceptada únicamente si se reconoce que
la de \[Newton] era errónea''.
% ``Einstein's theory can be accepted only with the recognition that
%   Newton's was wrong.'' \(The Structure of Scientific Revolutions),
%   Second Edition Enlarged, pg. 98.
% ``But the physical referents of these Einstenian concepts
%   [space, time, and mass] are by no means identical with those of
%   the Newtonian concepts that bear the same name.'' Ibid. pg. 102.

Lo de que \[Einstein] generalizó a \[Newton] es una mentira piadosa que
tiene que inventarse la ciencia materialista para no tener que reconocer
una incoherencia, a saber, que casi toda la ^ingeniería^, y mucha
^física^, sigue empleando la mecánica clásica de \[Newton]. Pero sólo es
incoherente si se cree que las verdaderas ^leyes^ de la naturaleza rigen
el ^universo^ material, y que cualesquiera otras leyes, por acertadas
que hayan parecido durante un tiempo, son simple y llanamente falsas.

El ^materialismo^, al negar la ^teoría^, prohibe que las ^leyes^ de la
naturaleza sean teoría. Para el materialismo, las leyes verdaderas
gobiernan el universo material, así que son una propiedad muy principal
de la materia. Pero las leyes erróneas, al no tener relación alguna con
la materia, no son nada de nada, y no hay justificación alguna que pueda
legitimar su uso.

Para nosotros, los ^subjetivistas^, las teorías son meros compendios de
^conocimiento^, y puesto que la mecánica clásica emplea cálculos mucho
más sencillos que la relativista, y sirve para ^predecir^ y controlar
muchos fenómenos, es inteligente usarla en muchas situaciones. Para el
subjetivismo no hay incoherencia, y el uso de la mecánica clásica con
preferencia a la relativista es sencillamente una cuestión de
oportunidad y criterio.


\endinput
